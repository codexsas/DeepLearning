{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep network models\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "nonlinearity = partial(F.relu, inplace=True)\n",
    "\n",
    "\n",
    "class SPPblock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SPPblock, self).__init__()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=[3, 3], stride=3)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=[5, 5], stride=5)\n",
    "        # self.pool4 = nn.MaxPool2d(kernel_size=[6, 6], stride=6)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.in_channels, h, w = x.size(1), x.size(2), x.size(3)\n",
    "\n",
    "        self.layer1 = F.upsample(self.conv(self.pool1(x)), size=(h, w), mode=\"bilinear\")\n",
    "        self.layer2 = F.upsample(self.conv(self.pool2(x)), size=(h, w), mode=\"bilinear\")\n",
    "        self.layer3 = F.upsample(self.conv(self.pool3(x)), size=(h, w), mode=\"bilinear\")\n",
    "        # self.layer4 = F.upsample(self.conv(self.pool4(x)), size=(h, w), mode=\"bilinear\")\n",
    "\n",
    "        out = torch.cat([self.layer1, self.layer2, self.layer3,\n",
    "                         # self.layer4,\n",
    "                         x], 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HDCblock(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(HDCblock, self).__init__()\n",
    "        self.dilate1 = nn.Conv2d(channel, channel, kernel_size=3, dilation=1, padding=1)\n",
    "        self.dilate2 = nn.Conv2d(channel, channel, kernel_size=3, dilation=2, padding=2)\n",
    "        self.dilate3 = nn.Conv2d(channel, channel, kernel_size=3, dilation=4, padding=4)\n",
    "        # self.dilate4 = nn.Conv2d(channel, channel, kernel_size=3, dilation=8, padding=8)\n",
    "        # self.dilate5 = nn.Conv2d(channel, channel, kernel_size=3, dilation=16, padding=16)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        dilate1_out = nonlinearity(self.dilate1(x))\n",
    "        dilate2_out = nonlinearity(self.dilate2(dilate1_out))\n",
    "        dilate3_out = nonlinearity(self.dilate3(dilate2_out))\n",
    "        # dilate4_out = nonlinearity(self.dilate4(dilate3_out))\n",
    "        # dilate5_out = nonlinearity(self.dilate5(dilate4_out))\n",
    "        out = x + dilate1_out + dilate2_out + dilate3_out  # + dilate4_out + dilate5_out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
    "        self.relu1 = nonlinearity\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n",
    "        self.relu2 = nonlinearity\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
    "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
    "        self.relu3 = nonlinearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPLinkNet34(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DPLinkNet34, self).__init__()\n",
    "\n",
    "        filters = [64, 128, 256, 512]\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.firstconv = resnet.conv1\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "\n",
    "        self.dblock = HDCblock(filters[3])\n",
    "        self.spp = SPPblock(filters[3])\n",
    "\n",
    "        self.decoder4 = DecoderBlock(filters[3] + 3, filters[2])\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
    "        self.finalrelu1 = nonlinearity\n",
    "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.finalrelu2 = nonlinearity\n",
    "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.firstconv(x)\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        x = self.firstmaxpool(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        # Center\n",
    "        e4 = self.dblock(e4)\n",
    "        e4 = self.spp(e4)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.decoder4(e4) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "\n",
    "        out = self.finaldeconv1(d1)\n",
    "        out = self.finalrelu1(out)\n",
    "        out = self.finalconv2(out)\n",
    "        out = self.finalrelu2(out)\n",
    "        out = self.finalconv3(out)\n",
    "\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "\n",
    "class DLinkNet34_less_pool(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DLinkNet34_less_pool, self).__init__()\n",
    "\n",
    "        filters = [64, 128, 256, 512]\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.firstconv = resnet.conv1\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "\n",
    "        self.dblock = HDCblock(filters[2])\n",
    "\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
    "        self.finalrelu1 = nonlinearity\n",
    "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.finalrelu2 = nonlinearity\n",
    "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.firstconv(x)\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        x = self.firstmaxpool(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "\n",
    "        # Center\n",
    "        e3 = self.dblock(e3)\n",
    "\n",
    "        # Decoder\n",
    "        d3 = self.decoder3(e3) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "\n",
    "        # Final Classification\n",
    "        out = self.finaldeconv1(d1)\n",
    "        out = self.finalrelu1(out)\n",
    "        out = self.finalconv2(out)\n",
    "        out = self.finalrelu2(out)\n",
    "        out = self.finalconv3(out)\n",
    "\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "\n",
    "class DLinkNet34(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DLinkNet34, self).__init__()\n",
    "\n",
    "        filters = [64, 128, 256, 512]\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.firstconv = resnet.conv1\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "\n",
    "        self.dblock = HDCblock(filters[3])\n",
    "\n",
    "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
    "        self.finalrelu1 = nonlinearity\n",
    "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.finalrelu2 = nonlinearity\n",
    "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.firstconv(x)\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        x = self.firstmaxpool(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        # Center\n",
    "        e4 = self.dblock(e4)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.decoder4(e4) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "\n",
    "        out = self.finaldeconv1(d1)\n",
    "        out = self.finalrelu1(out)\n",
    "        out = self.finalconv2(out)\n",
    "        out = self.finalrelu2(out)\n",
    "        out = self.finalconv3(out)\n",
    "\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "\n",
    "class DLinkNet50(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DLinkNet50, self).__init__()\n",
    "\n",
    "        filters = [256, 512, 1024, 2048]\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.firstconv = resnet.conv1\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "\n",
    "        self.dblock = HDCblock(filters[3])\n",
    "\n",
    "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
    "        self.finalrelu1 = nonlinearity\n",
    "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.finalrelu2 = nonlinearity\n",
    "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.firstconv(x)\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        x = self.firstmaxpool(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        # Center\n",
    "        e4 = self.dblock(e4)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.decoder4(e4) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "        out = self.finaldeconv1(d1)\n",
    "        out = self.finalrelu1(out)\n",
    "        out = self.finalconv2(out)\n",
    "        out = self.finalrelu2(out)\n",
    "        out = self.finalconv3(out)\n",
    "\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "\n",
    "class DLinkNet101(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DLinkNet101, self).__init__()\n",
    "\n",
    "        filters = [256, 512, 1024, 2048]\n",
    "        resnet = models.resnet101(pretrained=True)\n",
    "        self.firstconv = resnet.conv1\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "\n",
    "        self.dblock = HDCblock(filters[3])\n",
    "\n",
    "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
    "        self.finalrelu1 = nonlinearity\n",
    "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.finalrelu2 = nonlinearity\n",
    "        self.finalconv3 = nn.Conv2d(32, num_classes, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.firstconv(x)\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        x = self.firstmaxpool(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        # Center\n",
    "        e4 = self.dblock(e4)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.decoder4(e4) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "        out = self.finaldeconv1(d1)\n",
    "        out = self.finalrelu1(out)\n",
    "        out = self.finalconv2(out)\n",
    "        out = self.finalrelu2(out)\n",
    "        out = self.finalconv3(out)\n",
    "\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "\n",
    "class LinkNet34(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(LinkNet34, self).__init__()\n",
    "\n",
    "        filters = [64, 128, 256, 512]\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.firstconv = resnet.conv1\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "\n",
    "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n",
    "        self.finalrelu1 = nonlinearity\n",
    "        self.finalconv2 = nn.Conv2d(32, 32, 3)\n",
    "        self.finalrelu2 = nonlinearity\n",
    "        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.firstconv(x)\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        x = self.firstmaxpool(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.decoder4(e4) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "        out = self.finaldeconv1(d1)\n",
    "        out = self.finalrelu1(out)\n",
    "        out = self.finalconv2(out)\n",
    "        out = self.finalrelu2(out)\n",
    "        out = self.finalconv3(out)\n",
    "\n",
    "        return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dkceWAZ7hy7L"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "TILE_SIZE = 128\n",
    "PADDING_SIZE = 21  # round(TILE_SIZE / 4)\n",
    "\n",
    "LEFT_EDGE = -2\n",
    "TOP_EDGE = -1\n",
    "MIDDLE = 0\n",
    "RIGHT_EDGE = 1\n",
    "BOTTOM_EDGE = 2\n",
    "\n",
    "\n",
    "def get_patches(img, patch_h=TILE_SIZE, patch_w=TILE_SIZE):\n",
    "    y_stride, x_stride, = patch_h - (2 * PADDING_SIZE), patch_w - (2 * PADDING_SIZE)\n",
    "    if (patch_h > img.shape[0]) or (patch_w > img.shape[1]):\n",
    "        print(\"Invalid cropping: Cropping dimensions larger than image shapes (%r x %r with %r)\" % (\n",
    "        patch_h, patch_w, img.shape))\n",
    "        exit(1)\n",
    "\n",
    "    locations, patches = [], []\n",
    "    y = 0\n",
    "    y_done = False\n",
    "    while y <= img.shape[0] and not y_done:\n",
    "        x = 0\n",
    "        if y + patch_h > img.shape[0]:\n",
    "            y = img.shape[0] - patch_h\n",
    "            y_done = True\n",
    "        x_done = False\n",
    "        while x <= img.shape[1] and not x_done:\n",
    "            if x + patch_w > img.shape[1]:\n",
    "                x = img.shape[1] - patch_w\n",
    "                x_done = True\n",
    "            locations.append(((y, x, y + patch_h, x + patch_w),\n",
    "                              (y + PADDING_SIZE, x + PADDING_SIZE, y + y_stride, x + x_stride),\n",
    "                              TOP_EDGE if y == 0 else (BOTTOM_EDGE if y == (img.shape[0] - patch_h) else MIDDLE),\n",
    "                              LEFT_EDGE if x == 0 else (RIGHT_EDGE if x == (img.shape[1] - patch_w) else MIDDLE)))\n",
    "            patches.append(img[y:y + patch_h, x:x + patch_w, :])\n",
    "            x += x_stride\n",
    "        y += y_stride\n",
    "\n",
    "    return locations, patches\n",
    "\n",
    "\n",
    "def stitch_together(locations, patches, size, patch_h=TILE_SIZE, patch_w=TILE_SIZE):\n",
    "    output = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "    for location, patch in zip(locations, patches):\n",
    "        outer_bounding_box, inner_bounding_box, y_type, x_type = location\n",
    "        y_paste, x_paste, y_cut, x_cut, height_paste, width_paste = -1, -1, -1, -1, -1, -1\n",
    "        # print outer_bounding_box, inner_bounding_box, y_type, x_type\n",
    "\n",
    "        if y_type == TOP_EDGE:\n",
    "            y_cut = 0\n",
    "            y_paste = 0\n",
    "            height_paste = patch_h - PADDING_SIZE\n",
    "        elif y_type == MIDDLE:\n",
    "            y_cut = PADDING_SIZE\n",
    "            y_paste = inner_bounding_box[0]\n",
    "            height_paste = patch_h - 2 * PADDING_SIZE\n",
    "        elif y_type == BOTTOM_EDGE:\n",
    "            y_cut = PADDING_SIZE\n",
    "            y_paste = inner_bounding_box[0]\n",
    "            height_paste = patch_h - PADDING_SIZE\n",
    "\n",
    "        if x_type == LEFT_EDGE:\n",
    "            x_cut = 0\n",
    "            x_paste = 0\n",
    "            width_paste = patch_w - PADDING_SIZE\n",
    "        elif x_type == MIDDLE:\n",
    "            x_cut = PADDING_SIZE\n",
    "            x_paste = inner_bounding_box[1]\n",
    "            width_paste = patch_w - 2 * PADDING_SIZE\n",
    "        elif x_type == RIGHT_EDGE:\n",
    "            x_cut = PADDING_SIZE\n",
    "            x_paste = inner_bounding_box[1]\n",
    "            width_paste = patch_w - PADDING_SIZE\n",
    "\n",
    "        # print (y_paste, x_paste), (height_paste, width_paste), (y_cut, x_cut)\n",
    "        output[y_paste:y_paste + height_paste, x_paste:x_paste + width_paste] = patch[y_cut:y_cut + height_paste,\n",
    "                                                                                x_cut:x_cut + width_paste]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mXtA0Xx3ibLx"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and data loader\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "def randomHueSaturationValue(image,\n",
    "                             hue_shift_limit=(-180, 180),\n",
    "                             sat_shift_limit=(-255, 255),\n",
    "                             val_shift_limit=(-255, 255), u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(image)\n",
    "        hue_shift = np.random.randint(hue_shift_limit[0], hue_shift_limit[1] + 1)\n",
    "        hue_shift = np.uint8(hue_shift)\n",
    "        h += hue_shift\n",
    "        sat_shift = np.random.uniform(sat_shift_limit[0], sat_shift_limit[1])\n",
    "        s = cv2.add(s, sat_shift)\n",
    "        val_shift = np.random.uniform(val_shift_limit[0], val_shift_limit[1])\n",
    "        v = cv2.add(v, val_shift)\n",
    "        image = cv2.merge((h, s, v))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def randomShiftScaleRotate(image, mask,\n",
    "                           shift_limit=(-0.0, 0.0),\n",
    "                           scale_limit=(-0.0, 0.0),\n",
    "                           rotate_limit=(-0.0, 0.0),\n",
    "                           aspect_limit=(-0.0, 0.0),\n",
    "                           borderMode=cv2.BORDER_CONSTANT, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        height, width, channel = image.shape\n",
    "\n",
    "        angle = np.random.uniform(rotate_limit[0], rotate_limit[1])\n",
    "        scale = np.random.uniform(1 + scale_limit[0], 1 + scale_limit[1])\n",
    "        aspect = np.random.uniform(1 + aspect_limit[0], 1 + aspect_limit[1])\n",
    "        sx = scale * aspect / (aspect ** 0.5)\n",
    "        sy = scale / (aspect ** 0.5)\n",
    "        dx = round(np.random.uniform(shift_limit[0], shift_limit[1]) * width)\n",
    "        dy = round(np.random.uniform(shift_limit[0], shift_limit[1]) * height)\n",
    "\n",
    "        cc = np.math.cos(angle / 180 * np.math.pi) * sx\n",
    "        ss = np.math.sin(angle / 180 * np.math.pi) * sy\n",
    "        rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n",
    "\n",
    "        box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])\n",
    "        box1 = box0 - np.array([width / 2, height / 2])\n",
    "        box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2 + dx, height / 2 + dy])\n",
    "\n",
    "        box0 = box0.astype(np.float32)\n",
    "        box1 = box1.astype(np.float32)\n",
    "        mat = cv2.getPerspectiveTransform(box0, box1)\n",
    "        image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
    "                                    borderValue=(0, 0, 0,))\n",
    "        mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
    "                                   borderValue=(0, 0, 0,))\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def randomHorizontalFlip(image, mask, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        image = cv2.flip(image, 1)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def randomVerticleFlip(image, mask, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        image = cv2.flip(image, 0)\n",
    "        mask = cv2.flip(mask, 0)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def randomRotate90(image, mask, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        image = np.rot90(image)\n",
    "        mask = np.rot90(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def default_loader(id, root):\n",
    "    img = cv2.imread(os.path.join(root, \"{}_img.png\").format(id))\n",
    "    mask = cv2.imread(os.path.join(root + \"{}_mask.png\").format(id), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img = randomHueSaturationValue(img,\n",
    "                                   hue_shift_limit=(-30, 30),\n",
    "                                   sat_shift_limit=(-5, 5),\n",
    "                                   val_shift_limit=(-15, 15))\n",
    "\n",
    "    img, mask = randomShiftScaleRotate(img, mask,\n",
    "                                       shift_limit=(-0.1, 0.1),\n",
    "                                       scale_limit=(-0.1, 0.1),\n",
    "                                       aspect_limit=(-0.1, 0.1),\n",
    "                                       rotate_limit=(-0, 0))\n",
    "    img, mask = randomHorizontalFlip(img, mask)\n",
    "    img, mask = randomVerticleFlip(img, mask)\n",
    "    img, mask = randomRotate90(img, mask)\n",
    "\n",
    "    mask = np.expand_dims(mask, axis=2)\n",
    "    img = np.array(img, np.float32).transpose(2, 0, 1) / 255.0 * 3.2 - 1.6\n",
    "    mask = np.array(mask, np.float32).transpose(2, 0, 1) / 255.0\n",
    "    mask[mask >= 0.5] = 1\n",
    "    mask[mask < 0.5] = 0\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "class ImageFolder(data.Dataset):\n",
    "\n",
    "    def __init__(self, trainlist, root):\n",
    "        self.ids = trainlist\n",
    "        self.loader = default_loader\n",
    "        self.root = root\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        img, mask = self.loader(id, self.root)\n",
    "        img = torch.Tensor(img)\n",
    "        mask = torch.Tensor(mask)\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "lbQsBfUEihUp",
    "outputId": "1070dd62-0b2f-42c2-ea18-546a15cecee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patch size: 256 x 256\n",
      "Now processing image: C:\\Users\\MES\\Downloads\\project_dataset\\training\\H01_nine.bmp\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2549925c2f35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# extract the patches from the original document images and the corresponding ground truths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mimg_patch_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_patches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_patches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTILE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTILE_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mmsk_patch_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsk_patches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_patches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTILE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTILE_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Patches extracted:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_patches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-0ea1d1d26d58>\u001b[0m in \u001b[0;36mget_patches\u001b[1;34m(img, patch_h, patch_w)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_patches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatch_h\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTILE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatch_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTILE_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0my_stride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_stride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatch_h\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mPADDING_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatch_w\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mPADDING_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpatch_h\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpatch_w\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         print(\"Invalid cropping: Cropping dimensions larger than image shapes (%r x %r with %r)\" % (\n\u001b[0;32m     17\u001b[0m         patch_h, patch_w, img.shape))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for training\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "#from utils import get_patches\n",
    "\n",
    "TILE_SIZE = 256\n",
    "print(\"Image patch size:\", TILE_SIZE, \"x\", TILE_SIZE)\n",
    "\n",
    "data_root = r\"C:\\Users\\MES\\Downloads\\project_dataset\\training\"\n",
    "img_list = os.listdir(data_root)\n",
    "img_list.sort()\n",
    "\n",
    "data_train_dir = os.path.join(data_root, \"train\")\n",
    "if not os.path.exists(data_train_dir):\n",
    "    os.makedirs(data_train_dir)\n",
    "\n",
    "# img_patches, msk_patches = [], []  # patches for each image or ground truth\n",
    "total_img_patches, total_msk_patches = [], []  # patches for all the images or ground truths\n",
    "\n",
    "for idx in range(len(img_list)):\n",
    "    if os.path.isdir(os.path.join(data_root, img_list[idx])):\n",
    "        continue\n",
    "\n",
    "    print(\"Now processing image:\", os.path.join(data_root, img_list[idx]))\n",
    "    (fname, fext) = os.path.splitext(img_list[idx])\n",
    "    img = cv2.imread(os.path.join(data_root, img_list[idx]))\n",
    "    msk = cv2.imread(os.path.join(data_root, \"groundtruth\", fname + \"_GT.tiff\"))\n",
    "\n",
    "    # extract the patches from the original document images and the corresponding ground truths\n",
    "    img_patch_locations, img_patches = get_patches(img, TILE_SIZE, TILE_SIZE)\n",
    "    msk_patch_locations, msk_patches = get_patches(msk, TILE_SIZE, TILE_SIZE)\n",
    "\n",
    "    print(\"Patches extracted:\", len(img_patches))\n",
    "    for idy in range(len(img_patches)):\n",
    "        total_img_patches.append(img_patches[idy])\n",
    "        total_msk_patches.append(msk_patches[idy])\n",
    "\n",
    "print(\"Total number of train patches:\", len(total_img_patches))\n",
    "for idz in range(len(total_img_patches)):\n",
    "    cv2.imwrite(os.path.join(data_train_dir, str(idz) + \"_img.png\"), total_img_patches[idz])\n",
    "    cv2.imwrite(os.path.join(data_train_dir, str(idz) + \"_mask.png\"), total_msk_patches[idz])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-07436e10482e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMyFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdice_bce_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from data import ImageFolder\n",
    "from framework import MyFrame\n",
    "from loss import dice_bce_loss\n",
    "\n",
    "# from networks.unet import UNet\n",
    "# from networks.dunet import DUNet\n",
    "from networks.dplinknet import LinkNet34, DLinkNet34, DPLinkNet34\n",
    "\n",
    "SHAPE = (256, 256)\n",
    "DATA_NAME = \"DIBCO\"  # BickleyDiary, DIBCO, PLM\n",
    "DEEP_NETWORK_NAME = \"DPLinkNet34\"\n",
    "print(\"Now training dataset: {}, using network model: {}\".format(DATA_NAME, DEEP_NETWORK_NAME))\n",
    "\n",
    "train_root = \"./dataset/train/\"\n",
    "imagelist = list(filter(lambda x: x.find(\"img\") != -1, os.listdir(train_root)))\n",
    "trainlist = list(map(lambda x: x[:-8], imagelist))\n",
    "log_name = DATA_NAME.lower() + \"_\" + DEEP_NETWORK_NAME.lower()\n",
    "\n",
    "BATCHSIZE_PER_CARD = 32\n",
    "\n",
    "if DEEP_NETWORK_NAME == \"DPLinkNet34\":\n",
    "    solver = MyFrame(DPLinkNet34, dice_bce_loss, 2e-4)\n",
    "elif DEEP_NETWORK_NAME == \"DLinkNet34\":\n",
    "    solver = MyFrame(DLinkNet34, dice_bce_loss, 2e-4)\n",
    "elif DEEP_NETWORK_NAME == \"LinkNet34\":\n",
    "    solver = MyFrame(LinkNet34, dice_bce_loss, 2e-4)\n",
    "else:\n",
    "    print(\"Deep network not found, please have a check!\")\n",
    "    exit(0)\n",
    "\n",
    "batchsize = torch.cuda.device_count() * BATCHSIZE_PER_CARD\n",
    "\n",
    "dataset = ImageFolder(trainlist, train_root)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batchsize,\n",
    "    shuffle=True,\n",
    "    num_workers=4)\n",
    "\n",
    "mylog = open(\"logs/\" + log_name + \".log\", \"w\")\n",
    "no_optim = 0\n",
    "total_epoch = 500\n",
    "train_epoch_best_loss = 100.\n",
    "\n",
    "tic = time()\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    data_loader_iter = iter(data_loader)\n",
    "    train_epoch_loss = 0\n",
    "    for img, mask in data_loader_iter:\n",
    "        solver.set_input(img, mask)\n",
    "        train_loss = solver.optimize()\n",
    "        train_epoch_loss += train_loss\n",
    "    train_epoch_loss /= len(data_loader_iter)\n",
    "    print(\"********\", file=mylog)\n",
    "    print(\"epoch:\", epoch, \"    time:\", int(time() - tic), file=mylog)\n",
    "    print(\"train_loss:\", train_epoch_loss, file=mylog)\n",
    "    print(\"SHAPE:\", SHAPE, file=mylog)\n",
    "    print(\"********\")\n",
    "    print(\"epoch:\", epoch, \"    time:\", int(time() - tic))\n",
    "    print(\"train_loss:\", train_epoch_loss)\n",
    "    print(\"SHAPE:\", SHAPE)\n",
    "\n",
    "    if train_epoch_loss >= train_epoch_best_loss:\n",
    "        no_optim += 1\n",
    "    else:\n",
    "        no_optim = 0\n",
    "        train_epoch_best_loss = train_epoch_loss\n",
    "        solver.save(\"weights/\" + log_name + \".th\")\n",
    "\n",
    "    if no_optim > 20:\n",
    "        print(\"early stop at %d epoch\" % epoch, file=mylog)\n",
    "        print(\"early stop at %d epoch\" % epoch)\n",
    "        break\n",
    "\n",
    "    if no_optim > 10:\n",
    "        if solver.old_lr < 1e-7:\n",
    "            break\n",
    "        solver.load(\"weights/\" + log_name + \".th\")\n",
    "        solver.update_lr(5.0, factor=True, mylog=mylog)\n",
    "    mylog.flush()\n",
    "\n",
    "print(\"Finish!\", file=mylog)\n",
    "print(\"Finish!\")\n",
    "mylog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "# from networks.unet import UNet\n",
    "# from networks.dunet import DUNet\n",
    "from networks.dplinknet import LinkNet34, DLinkNet34, DPLinkNet34\n",
    "from utils import get_patches, stitch_together\n",
    "\n",
    "BATCHSIZE_PER_CARD = 32\n",
    "\n",
    "\n",
    "class TTAFrame():\n",
    "    def __init__(self, net):\n",
    "        self.net = net().cuda()\n",
    "        self.net = torch.nn.DataParallel(self.net, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "    def test_one_img_from_path(self, path, evalmode=True):\n",
    "        if evalmode:\n",
    "            self.net.eval()\n",
    "        batchsize = torch.cuda.device_count() * BATCHSIZE_PER_CARD\n",
    "        if batchsize >= 8:\n",
    "            return self.test_one_img_from_path_1(path)\n",
    "        elif batchsize >= 4:\n",
    "            return self.test_one_img_from_path_2(path)\n",
    "        elif batchsize >= 2:\n",
    "            return self.test_one_img_from_path_4(path)\n",
    "\n",
    "    def test_one_img_from_path_8(self, path):\n",
    "        img = np.array(path)  # .transpose(2,0,1)[None]\n",
    "        # img = cv2.imread(path)  # .transpose(2,0,1)[None]\n",
    "        img90 = np.array(np.rot90(img))\n",
    "        img1 = np.concatenate([img[None], img90[None]])\n",
    "        img2 = np.array(img1)[:, ::-1]\n",
    "        img3 = np.array(img1)[:, :, ::-1]\n",
    "        img4 = np.array(img2)[:, :, ::-1]\n",
    "\n",
    "        img1 = img1.transpose(0, 3, 1, 2)\n",
    "        img2 = img2.transpose(0, 3, 1, 2)\n",
    "        img3 = img3.transpose(0, 3, 1, 2)\n",
    "        img4 = img4.transpose(0, 3, 1, 2)\n",
    "\n",
    "        img1 = V(torch.Tensor(np.array(img1, np.float32) / 255.0 * 3.2 - 1.6).cuda())\n",
    "        img2 = V(torch.Tensor(np.array(img2, np.float32) / 255.0 * 3.2 - 1.6).cuda())\n",
    "        img3 = V(torch.Tensor(np.array(img3, np.float32) / 255.0 * 3.2 - 1.6).cuda())\n",
    "        img4 = V(torch.Tensor(np.array(img4, np.float32) / 255.0 * 3.2 - 1.6).cuda())\n",
    "\n",
    "        maska = self.net.forward(img1).squeeze().cpu().data.numpy()\n",
    "        maskb = self.net.forward(img2).squeeze().cpu().data.numpy()\n",
    "        maskc = self.net.forward(img3).squeeze().cpu().data.numpy()\n",
    "        maskd = self.net.forward(img4).squeeze().cpu().data.numpy()\n",
    "\n",
    "        mask1 = maska + maskb[:, ::-1] + maskc[:, :, ::-1] + maskd[:, ::-1, ::-1]\n",
    "        mask2 = mask1[0] + np.rot90(mask1[1])[::-1, ::-1]\n",
    "\n",
    "        return mask2\n",
    "\n",
    "    def test_one_img_from_path_4(self, path):\n",
    "        img = np.array(path)  # .transpose(2,0,1)[None]\n",
    "        # img = cv2.imread(path)  # .transpose(2,0,1)[None]\n",
    "        img90 = np.array(np.rot90(img))\n",
    "        img1 = np.concatenate([img[None], img90[None]])\n",
    "        img2 = np.array(img1)[:, ::-1]\n",
    "        img3 = np.array(img1)[:, :, ::-1]\n",
    "        img4 = np.array(img2)[:, :, ::-1]\n",
    "\n",
    "        img1 = img1.transpose(0, 3, 1, 2)\n",
    "        img2 = img2.transpose(0, 3, 1, 2)\n",
    "        img3 = img3.transpose(0, 3, 1, 2)\n",
    "        img4 = img4.transpose(0, 3, 1, 2)\n",
    "\n",
    "        img1 = V(torch.Tensor(np.array(img1, np.float32) / 255.0 * 3.2 - 1.6).cuda())\n",
    "        img2 = V(torch.Tensor(np.array(img2, np.float32) / 255.0 * 3.2 - 1.6).cuda())\n",
    "        img3 = V(torch.Tensor(np.array(img3, np.float32) / 255.0 * 3.2 - 1.6).cuda())\n",
    "        img4 = V(torch.Tensor(np.array(img4, np.float32) / 255.0 * 3.2 - 1.6).cuda())\n",
    "\n",
    "        maska = self.net.forward(img1).squeeze().cpu().data.numpy()\n",
    "        maskb = self.net.forward(img2).squeeze().cpu().data.numpy()\n",
    "        maskc = self.net.forward(img3).squeeze().cpu().data.numpy()\n",
    "        maskd = self.net.forward(img4).squeeze().cpu().data.numpy()\n",
    "\n",
    "        mask1 = maska + maskb[:, ::-1] + maskc[:, :, ::-1] + maskd[:, ::-1, ::-1]\n",
    "        mask2 = mask1[0] + np.rot90(mask1[1])[::-1, ::-1]\n",
    "\n",
    "        return mask2\n",
    "\n",
    "    def test_one_img_from_path_2(self, path):\n",
    "        img = np.array(path)  # .transpose(2,0,1)[None]\n",
    "        # img = cv2.imread(path)  # .transpose(2,0,1)[None]\n",
    "        img90 = np.array(np.rot90(img))\n",
    "        img1 = np.concatenate([img[None], img90[None]])\n",
    "        img2 = np.array(img1)[:, ::-1]\n",
    "        img3 = np.concatenate([img1, img2])\n",
    "        img4 = np.array(img3)[:, :, ::-1]\n",
    "        img5 = img3.transpose(0, 3, 1, 2)\n",
    "        img5 = np.array(img5, np.float32) / 255.0 * 3.2 - 1.6\n",
    "        img5 = V(torch.Tensor(img5).cuda())\n",
    "        img6 = img4.transpose(0, 3, 1, 2)\n",
    "        img6 = np.array(img6, np.float32) / 255.0 * 3.2 - 1.6\n",
    "        img6 = V(torch.Tensor(img6).cuda())\n",
    "\n",
    "        maska = self.net.forward(img5).squeeze().cpu().data.numpy()  # .squeeze(1)\n",
    "        maskb = self.net.forward(img6).squeeze().cpu().data.numpy()\n",
    "\n",
    "        mask1 = maska + maskb[:, :, ::-1]\n",
    "        mask2 = mask1[:2] + mask1[2:, ::-1]\n",
    "        mask3 = mask2[0] + np.rot90(mask2[1])[::-1, ::-1]\n",
    "\n",
    "        return mask3\n",
    "\n",
    "    def test_one_img_from_path_1(self, path):\n",
    "        img = np.array(path)  # .transpose(2,0,1)[None]\n",
    "        # img = cv2.imread(path)  # .transpose(2,0,1)[None]\n",
    "        img90 = np.array(np.rot90(img))\n",
    "        img1 = np.concatenate([img[None], img90[None]])\n",
    "        img2 = np.array(img1)[:, ::-1]\n",
    "        img3 = np.concatenate([img1, img2])\n",
    "        img4 = np.array(img3)[:, :, ::-1]\n",
    "        img5 = np.concatenate([img3, img4]).transpose(0, 3, 1, 2)\n",
    "        img5 = np.array(img5, np.float32) / 255.0 * 3.2 - 1.6\n",
    "        img5 = V(torch.Tensor(img5).cuda())\n",
    "\n",
    "        mask = self.net.forward(img5).squeeze().cpu().data.numpy()  # .squeeze(1)\n",
    "        mask1 = mask[:4] + mask[4:, :, ::-1]\n",
    "        mask2 = mask1[:2] + mask1[2:, ::-1]\n",
    "        mask3 = mask2[0] + np.rot90(mask2[1])[::-1, ::-1]\n",
    "\n",
    "        return mask3\n",
    "\n",
    "    def load(self, path):\n",
    "        self.net.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "TILE_SIZE = 256\n",
    "DATA_NAME = \"DIBCO\"  # BickleyDiary, DIBCO, PLM\n",
    "DEEP_NETWORK_NAME = \"DPLinkNet34\"  # LinkNet34, DLinkNet34, DPLinkNet34\n",
    "\n",
    "img_indir = \"dataset/test/\"\n",
    "print(\"Image input directory:\", img_indir)\n",
    "\n",
    "img_outdir = os.path.join(img_indir, \"Binarized\")\n",
    "if not os.path.exists(img_outdir):\n",
    "    os.makedirs(img_outdir)\n",
    "print(\"Image output directory:\", img_outdir)\n",
    "\n",
    "img_list = os.listdir(img_indir)\n",
    "img_list.sort()\n",
    "\n",
    "if DEEP_NETWORK_NAME == \"DPLinkNet34\":\n",
    "    solver = TTAFrame(DPLinkNet34)\n",
    "elif DEEP_NETWORK_NAME == \"DLinkNet34\":\n",
    "    solver = TTAFrame(DLinkNet34)\n",
    "elif DEEP_NETWORK_NAME == \"LinkNet34\":\n",
    "    solver = TTAFrame(LinkNet34)\n",
    "else:\n",
    "    print(\"Deep network not found, please have a check!\")\n",
    "    exit(0)\n",
    "# print(solver.net)\n",
    "# summary(solver.net, input_size=(3, TILE_SIZE, TILE_SIZE))  # summary(your_model, input_size=(channels, H, W))\n",
    "\n",
    "print(\"Now loading the model weights:\", \"weights/\" + DATA_NAME.lower() + \"_\" + DEEP_NETWORK_NAME.lower() + \".th\")\n",
    "solver.load(\"weights/\" + DATA_NAME.lower() + \"_\" + DEEP_NETWORK_NAME.lower() + \".th\")\n",
    "\n",
    "start_time = time()\n",
    "for idx in range(len(img_list)):\n",
    "    if os.path.isdir(os.path.join(img_indir, img_list[idx])):\n",
    "        continue\n",
    "\n",
    "    print(\"Now processing image:\", img_list[idx])\n",
    "    fname, fext = os.path.splitext(img_list[idx])\n",
    "    img_input = os.path.join(img_indir, img_list[idx])\n",
    "    img_output = os.path.join(img_outdir, fname + \"-\" + DEEP_NETWORK_NAME + \".tiff\")\n",
    "\n",
    "    img = cv2.imread(img_input)\n",
    "    locations, patches = get_patches(img, TILE_SIZE, TILE_SIZE)\n",
    "    masks = []\n",
    "    for idy in range(len(patches)):\n",
    "        msk = solver.test_one_img_from_path(patches[idy])\n",
    "        masks.append(msk)\n",
    "    prediction = stitch_together(locations, masks, tuple(img.shape[0:2]), TILE_SIZE, TILE_SIZE)\n",
    "    prediction[prediction >= 5.0] = 255\n",
    "    prediction[prediction < 5.0] = 0\n",
    "    # prediction = np.concatenate([prediction[:, :, None], prediction[:, :, None], prediction[:, :, None]], axis=2)\n",
    "    cv2.imwrite(img_output, prediction.astype(np.uint8))\n",
    "\n",
    "print(\"Total running time: %f sec.\" % (time() - start_time))\n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BINARY.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
